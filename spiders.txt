# -*- coding: utf-8 -*-

import scrapy

import sys
reload( sys )
sys.setdefaultencoding( "utf8" )

class CnblogsMySQL(scrapy.Spider):

   # 爬虫的名字，必须有这个变量
   name = 'cnblogs_mysql'

   page_index = 1

   # 初始地址，必须有这个变量
   start_urls = [
       'https://www.cnblogs.com/cate/mysql/' + str( page_index ),
   ]

   def parse(self, response):

       post_items = response.xpath( 
           "//div[@id='wrapper']/div[@id='main']/div[@id='post_list']/div[@class='post_item']/div[@class='post_item_body']" 
               )

       for post_item_body in post_items:
           yield {
               'article_title': 
                   post_item_body.xpath( "h3/a/text()" ).extract_first().strip(),
               'article_summary': 
                   post_item_body.xpath( "p[@class='post_item_summary']/text()" ).extract_first().strip(),
               'post_date': 
                   post_item_body.xpath( "div[@class='post_item_foot']/text()" ).extract()[ 1 ].strip(),
               'article_view' : 
                   post_item_body.xpath( 
                           "div[@class='post_item_foot']/span[@class='article_view']/a/text()" 
                       ).extract_first().strip()
           }

       next_page_url = None
       self.page_index += 1
       if self.page_index <= 20:
           next_page_url = "https://www.cnblogs.com/cate/mysql/" + str( self.page_index )
       else:
           next_page_url = None

       if next_page_url is not None:
           yield scrapy.Request(response.urljoin(next_page_url))这个就是我们的爬虫，其中name和start_urls两个变量必须存在。parse方法的作用是将响应内容解析为我们需要的数据。parse中的for循环就是在提取每一页中的20篇文章。解析并提取完成后，通过yield将结果抛到pipeline进行存储。2）修改pipelines.py文件，内容如下：# -*- coding: utf-8 -*-

# Define your item pipelines here
#
# Don't forget to add your pipeline to the ITEM_PIPELINES setting
# See: http://doc.scrapy.org/en/latest/topics/item-pipeline.html


class CnblogsScrapyPipeline(object):

   def open_spider( self, spider ):

       self.fp = open( "data.list", "w" )

   def close_spider( self, spider ):

       self.fp.close()

   def process_item(self, item, spider):

       self.fp.write( item[ "article_title" ] + "\n" )
       self.fp.write( item[ "article_view" ] + "\n" )
       self.fp.write( item[ "post_date" ] + "\n" )
       self.fp.write( item[ "article_summary" ] + "\n\n" )

       return item